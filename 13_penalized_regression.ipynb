{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R => Python  \n",
    "Source: https://web.stanford.edu/class/stats191/notebooks/Penalized_regression.html\n",
    "\n",
    "\n",
    "NOTE: \n",
    "* Not exactly porting R to Python.\n",
    "* Also not [rpy2](https://rpy2.readthedocs.io/en/version_2.8.x/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance tradeoff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.formula.api import glm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy; scipy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- One goal of a regression analysis is to \"build\" a model that predicts well -- AIC / $C_p$ & Cross-validation selection criteria based on this.\n",
    "\n",
    "- This is slightly different than the goal of making inferences about $\\beta$ that we've focused on so far.\n",
    "\n",
    "- What does \"predict well\" mean? \n",
    "$$\n",
    "\\begin{aligned}\n",
    "     MSE_{pop}({{\\cal M}}) &= {\\mathbb{E}}\\left((Y_{new} - \\widehat{Y}_{new,{\\cal M}}(X_{new}))^2\\right) \\\\\n",
    "     &=\n",
    "     {\\text{Var}}(Y_{new}) + {\\text{Var}}(\\widehat{Y}_{new,{\\cal M}}) +\n",
    "     \\\\\n",
    "     & \\qquad \\quad \\text{Bias}(\\widehat{Y}_{new,{\\cal M}})^2.\n",
    "     \\end{aligned}$$\n",
    " \n",
    "- Can we take an estimator for a model ${\\cal M}$ and make it better in terms of $MSE$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage estimators: one sample problem\n",
    "\n",
    "1. Generate $Y_{100 \\times 1} \\sim N(\\mu \\cdot 1, 5^2 I_{100 \\times 100})$, with $\\mu=0.5$.\n",
    "2. For $0 \\leq \\alpha \\leq 1$, set $\\hat{Y}(\\alpha) = \\alpha \\bar{Y}.$\n",
    "3. Compute $MSE(\\hat{Y}(\\alpha)) = \\frac{1}{100}\\sum_{i=1}^{100} (\\hat{Y}_{\\alpha} - 0.5)^2$\n",
    "4. Repeat 1000 times, plot average of $MSE(\\hat{Y}(\\alpha))$.\n",
    "\n",
    "**For what value of $\\alpha$ is $\\hat{Y}(\\alpha)$ unbiased?**\n",
    "\n",
    "**Is this the best estimate of $\\mu$ in terms of MSE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0.5\n",
    "sigma = 5\n",
    "nsample = 100\n",
    "ntrial = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse(mu_hat, mu):\n",
    "    return np.sum((mu_hat - mu)**2) / len(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.linspace(0, 1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias= (1 - alpha) * mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = (alpha**2) * 25/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = []\n",
    "for trial in range(ntrial):\n",
    "    result = []\n",
    "    z = stats.norm.rvs(size=nsample) * sigma + mu\n",
    "    for num in alpha:\n",
    "        result.append(get_mse(num * np.mean(z) * np.ones(nsample), mu * np.ones(nsample)))\n",
    "    mse.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_mean = np.mean(mse, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alpha, mse_mean, 'r')\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel('MSE(alpha)')\n",
    "plt.xlabel('Shrinkage factor, alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alpha, mse_mean, 'r')\n",
    "plt.plot(alpha, bias**2, 'g')\n",
    "plt.plot(alpha, variance, 'b')\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel('MSE(alpha)')\n",
    "plt.xlabel('Shrinkage factor, alpha')\n",
    "plt.legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Shrinkage & Penalties\n",
    "\n",
    "* Shrinkage can be thought of as \"constrained\" or \"penalized\" minimization.\n",
    "\n",
    "* Constrained form:\n",
    "$$\\text{minimize}_{\\mu} \\sum_{i=1}^n (Y_i - \\mu)^2 \\quad \\text{subject to $\\mu^2 \\leq C$}$$\n",
    "\n",
    "* Lagrange multiplier form: equivalent to \n",
    "$$\\widehat{\\mu}_{\\lambda} = \\text{argmin}_{\\mu} \\sum_{i=1}^n (Y_i - \\mu)^2 + \\lambda \\cdot \\mu^2$$ for some $\\lambda=\\lambda_C$.\n",
    "\n",
    "* As we vary $\\lambda$ we solve all versions of the constrained form.\n",
    "\n",
    "### Solving for $\\widehat{\\mu}_{\\lambda}$\n",
    "\n",
    "* Differentiating: $- 2 \\sum_{i=1}^n (Y_i - \\widehat{\\mu}_{\\lambda}) + 2 \\lambda \\widehat{\\mu}_{\\lambda} = 0$\n",
    "* Solving $\\widehat{\\mu}_{\\lambda} = \\frac{\\sum_{i=1}^n Y_i}{n + \\lambda} = \\frac{n}{n+\\lambda} \\overline{Y}.$\n",
    "* As $\\lambda \\rightarrow 0$, $\\widehat{\\mu}_{\\lambda} \\rightarrow {\\overline{Y}}.$\n",
    "* As $\\lambda \\rightarrow \\infty$ $\\widehat{\\mu}_{\\lambda} \\rightarrow 0.$\n",
    "\n",
    "** We see that $\\widehat{\\mu}_{\\lambda} = \\bar{Y} \\cdot \\left(\\frac{n}{n+\\lambda}\\right).$ **\n",
    "\n",
    "** In other words, considering all penalized estimators traces out the\n",
    "MSE curve above.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = nsample / alpha - nsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lam, mse_mean, 'r')\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel('MSE(alpha)')\n",
    "plt.xlabel('Penalty parameter, lambda')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lam, mse_mean, 'r')\n",
    "plt.plot(lam, bias**2, 'g')\n",
    "plt.plot(lam, variance, 'b')\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel('MSE(alpha)')\n",
    "plt.xlabel('Penalty parameter, lambda')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much to shrink?\n",
    "\n",
    "- In our one-sample example,\n",
    "- $$\\begin{aligned}\n",
    " MSE_{pop}(\\alpha) &=   {\\text{Var}}( \\alpha \\bar{Y}) + \\text{Bias}(\\alpha \\bar{Y})^2 +  \\text{Var}(Y_{new})\n",
    "\\\\\n",
    " &= \\frac{\\alpha^2 \\sigma^2}{n} + \\mu^2 (1 - \\alpha)^2 +  \\text{Var}(Y_{new}) \n",
    " \\end{aligned}$$\n",
    "- Differentiating and solving: \n",
    "$$\\begin{aligned}\n",
    " 0 &= -2 \\mu^2(1 - \\alpha^*) + 2 \\frac{\\alpha^* \\sigma^2}{n}  \\\\\n",
    " \\alpha^* & = \\frac{\\mu^2}{\\mu^2+\\sigma^2/n} = \\frac{(\\mu/(\\sigma/\\sqrt{n}))^2}{(\\mu/(\\sigma/\\sqrt{n}))^2 + 1} \\\\\n",
    " &= \\frac{0.5^2}{0.5^2+25/100} = 0.5\n",
    " \\end{aligned}$$\n",
    "     \n",
    "** We see that the optimal $\\alpha$ depends on the unknown $SNR=\\mu/(\\sigma/\\sqrt{n})$. Value is 1/8.**\n",
    "\n",
    "** In practice we might hope to estimate MSE with cross-validation.**\n",
    "\n",
    "Let's see how our theoretical choice matches the \n",
    "MSE on our 100 sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alpha, mse_mean, 'r')\n",
    "plt.ylim(bottom=0)\n",
    "plt.ylabel('MSE(alpha)')\n",
    "plt.xlabel('Shrinkage factor, alpha')\n",
    "plt.axvline(mu**2/(mu**2+sigma**2/nsample), linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalties & Priors\n",
    "\n",
    "- Minimizing $\\sum_{i=1}^n (Y_i - \\mu)^2 + \\lambda \\mu^2$ is similar to computing \"MLE\" of $\\mu$ if the likelihood was proportional to \n",
    "$$\\exp \\left(-\\frac{1}{2\\sigma^2}\\left(  \\|Y-\\mu\\|^2_2 + \\lambda \\mu^2\\right) \\right).$$\n",
    "\n",
    "- If $\\lambda=m$, an integer, then $\\widehat{\\mu}_{\\lambda}$ is the sample mean of $(Y_1, \\dots, Y_n,0 ,\\dots, 0) \\in \\mathbb{R}^{n+m}$.\n",
    "\n",
    "- This is equivalent to adding some data with $Y=0$. \n",
    "\n",
    "- To a Bayesian,\n",
    "this extra data is a *prior distribution* and we are computing the so-called\n",
    "*MAP* or posterior mode.\n",
    "\n",
    "## AIC as penalized regression\n",
    "\n",
    "- Model selection with $C_p$ (or AIC with $\\sigma^2$ assumed known)\n",
    "is a version of penalized regression.\n",
    "\n",
    "- The best subsets version of AIC (which is not exactly equivalent to *step*)\n",
    "$$\n",
    "\\hat{\\beta}_{AIC} = \\text{argmin}_{\\beta} \\frac{1}{\\sigma^2}\\|Y-X\\beta\\|^2_2 + 2 \\|\\beta\\|_0\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\|\\beta\\|_0 = \\#\\left\\{j : \\beta_j \\neq 0 \\right\\}\n",
    "$$\n",
    "is called the $\\ell_0$ norm.\n",
    "\n",
    "- The $\\ell_0$ penalty can be thought of as a measure of *complexity* of the model. Most penalties are similar versions of *complexity*.\n",
    "\n",
    "## Penalized regression in general\n",
    "\n",
    "* Not all biased models are better – we need a way to find \"good\" biased models.\n",
    "\n",
    "* Inference ($F$, $\\chi^2$ tests, etc) is not quite exact for biased models.\n",
    "Though, there has been some recent work to address the issue of [post-selection inference](http://arxiv.org/abs/1311.6238), at least for some penalized regression problems.\n",
    "\n",
    "* Heuristically, \"large $\\beta$\" (measured by some norm) is interpreted as \"complex model\". Goal is really to penalize \"complex\" models, i.e. Occam’s razor.\n",
    "* If truth really is complex, this may not work! (But, it will then be hard to build a good model anyways ... (statistical lore))\n",
    "\n",
    "## Ridge regression\n",
    "\n",
    "- Assume that columns $(X_j)_{1 \\leq j \\leq p}$ have zero mean, and SD 1 and $Y$ has zero mean.\n",
    "\n",
    "- This is called the *standardized model*.\n",
    "\n",
    "- The ridge estimator is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\beta}_{\\lambda} &= \\text{argmin}_{\\beta} \\frac{1}{2n}\\|Y-X\\beta\\|^2_2 + \\frac{\\lambda}{2} \\|\\beta\\|^2_2 \\\\\n",
    "&= \\text{argmin}_{\\beta} MSE_{\\lambda}(\\beta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "  \n",
    "- Corresponds (through Lagrange multiplier) to a quadratic constraint on ${\\beta_{}}$’s.\n",
    "\n",
    "- This is the natural generalization of the penalized\n",
    "version of our shrinkage estimator.\n",
    "\n",
    "### Solving the normal equations\n",
    "\n",
    "* Normal equations $$\\frac{\\partial}{\\partial {\\beta_{l}}} MSE_{\\lambda}({\\beta_{}}) = - \\frac{1}{n}  (Y - X{\\beta_{}})^TX_l  +  \\lambda {\\beta_{l}}$$\n",
    "* $$- \\frac{1}{n}(Y - X{\\widehat{\\beta}_{\\lambda}})^T X_l +  \\lambda {\\widehat{\\beta}_{l,\\lambda}} = 0, \\qquad 1 \\leq l \\leq p$$\n",
    "* In matrix form $$-\\frac{X^TY}{n} +  \\left(\\frac{X^TX}{n} + \\lambda I\\right) {\\widehat{\\beta}_{\\lambda}} = 0.$$\n",
    "* Or $${\\widehat{\\beta}_{\\lambda}} = \\left(\\frac{X^TX}{n} + \\lambda I\\right)^{-1} \\frac{X^TY}{n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "diabetes = datasets.load_diabetes()\n",
    "\"\"\"\n",
    "From Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \n",
    "\"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499, we have\n",
    "\n",
    "\"Ten baseline variables, age, sex, body mass index, average blood pressure, \n",
    "and six blood serum measurements were obtained for each of n = 442 diabetes patients, \n",
    "as well as the response of interest, a quantitative measure of disease progression one year \n",
    "after baseline.\"\n",
    "\n",
    "In the tab delimited file above, the variables are named\n",
    "\n",
    "    AGE SEX BMI BP S1 S2 S3 S4 S5 S6 Y\n",
    "\n",
    "reference: https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sklearn version -> Standardized: zero mean and unit L2 norm \n",
    "diabetes.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn version -> Standardized: zero mean and unit L2 norm \n",
    "diabetes.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_const = sm.add_constant(diabetes.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The following process and the plot is different from R notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The fraction of the penalty given to the L1 penalty term. \n",
    "# Must be between 0 and 1 (inclusive). \n",
    "# If 0, the fit is a ridge fit, if 1 it is a lasso fit.\n",
    "model = OLS(diabetes.target, data_with_const).fit_regularized(alpha=0.0, \n",
    "                                                            L1_wt=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![R_result](./img/13_lm_ridge_coef.png \"R result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The following process and the plot is different from R notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_lst = np.linspace(0, 0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for alpha in alpha_lst:\n",
    "    model = OLS(diabetes.target, data_with_const).fit_regularized(alpha=alpha, \n",
    "                                                            L1_wt=0.0)\n",
    "    result.append(model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(result, index=alpha_lst)\n",
    "ax = df.plot()\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The above process and the plot is different from R notebook**  \n",
    "It seems like to me that the plot in the R notebook used ```diabetes.ridge['coef']```, not ```coef(diabetes.ridge)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ```lm.ridge``` says (in describing the ```$coef``` element of the returned object) [emphasis added]\n",
    "\n",
    ">coef: matrix of coefficients, one row for each value of ‘lambda’. Note that these are not on the original scale and are for use by the ‘coef’ method.\n",
    "\n",
    "This means, specifically, that the ```$coef``` element is not intended for end-users (\"if you have to ask ...\"). (If you want to see how ```$coef``` is translated, inspect ```MASS:::coef.ridgelm```.) In general, it's better practice to use an accessor method such as ```coef()```, when it exists, than to extract components from the guts of a returned object using ```$``` (or ```@``` for S4 objects) - for exactly this reason. Package authors provide ```coef()``` methods for a reason ...\n",
    "\n",
    "[Source](https://stackoverflow.com/questions/34567594/masslm-ridge-coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $\\lambda$\n",
    "\n",
    "* If we knew $E[MSE_{\\lambda}]$ as a function of $\\lambda$ then we would simply choose the $\\lambda$ that minimizes it.\n",
    "* To do this, we need to estimate it.\n",
    "* A popular method is cross-validation as a function of $\\lambda$. Breaks the data up into smaller groups and uses part of the data to predict the rest.\n",
    "* We saw this in diagnostics (Cook’s distance measured the fit with and without each point in the data set) and model selection.\n",
    "\n",
    "### $K$-fold cross-validation for penalized model\n",
    "\n",
    "* Fix a model (i.e. fix $\\lambda$). Break data set into $K$ approximately equal sized groups $(G_1, \\dots, G_K)$.\n",
    "* for (i in 1:K)\n",
    "   Use all groups except $G_i$ to fit model, predict outcome in group $G_i$ based on this model $\\widehat{Y}_{j(i),\\lambda}, j \\in G_i$.\n",
    "* Estimate $CV(\\lambda) = \\frac{1}{n}\\sum_{i=1}^K \\sum_{j \\in G_i} (Y_j - \\widehat{Y}_{j(i),\\lambda})^2.$\n",
    "\n",
    "Here is a function to estimate the CV for our one parameter example. In practice, we only have one sample to form the CV curve. In this example below,\n",
    "I will compute the average CV error for 500 trials to show that it is roughly\n",
    "comparable in shape to the MSE curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python port for glmnet\n",
    "# https://web.stanford.edu/~hastie/glmnet_python/\n",
    "# https://github.com/bbalasub1/glmnet_python\n",
    "# Algorithm was designed by Jerome Friedman, Trevor Hastie and Rob Tibshirani. Fortran code was written by Jerome Friedman. R wrapper (from which the MATLAB wrapper was adapted) was written by Trevor Hastie.\n",
    "# The original MATLAB wrapper was written by Hui Jiang (14 Jul 2009), and was updated and is maintained by Junyang Qian (30 Aug 2013).\n",
    "# This python wrapper (which was adapted from the MATLAB and R wrappers) was originally written by B. J. Balakumar (5 Sep 2016).\n",
    "# List of other contributors along with a summary of their contributions is included in the contributors.dat file.\n",
    "# B. J. Balakumar, bbalasub@gmail.com (Sep 5, 2016). Department of Statistics, Stanford University, Stanford, CA\n",
    "\n",
    "# There are other glmnet pkgs..\n",
    "# https://github.com/civisanalytics/python-glmnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
